# Data Pre-processing Guide

## Overview

To accelerate training and validation, the project uses an offline pre-processing step to generate the probability and threshold maps required by the DBNet model. This avoids calculating these maps on-the-fly for every batch, which was a major performance bottleneck.

This process must be run once after the dataset is prepared.

## Why Pre-processing?

The original pipeline computed probability and threshold maps during training using the collate function. This approach had several issues:

- **Performance Bottleneck**: Map generation is computationally expensive (pyclipper operations, distance calculations)
- **Redundant Computation**: Same maps were computed repeatedly across epochs
- **Caching Challenges**: On-the-fly caching proved ineffective due to key collision issues and memory overhead

The offline pre-processing approach provides:

- **5-8x faster validation times**: Maps are pre-computed once and loaded from disk
- **Consistent quality**: All maps are generated with identical parameters
- **Simplified pipeline**: Collate function becomes a simple tensor stacker
- **Debugging ease**: Pre-computed maps can be inspected independently

## How to Run Pre-processing

The pre-processing logic is handled by the `scripts/preprocess_maps.py` script. It uses the project's Hydra configuration to ensure that all data transformations are consistent with the training and validation pipelines.

### Full Dataset Pre-processing

To run the script for the full dataset, execute the following command from the root of the project:

```bash
uv run python scripts/preprocess_maps.py
```

This will process both training and validation datasets using the limits specified in `configs/data/base.yaml`:
- Training: 2000 samples (configurable via `data.train_num_samples`)
- Validation: 200 samples (configurable via `data.val_num_samples`)

### Custom Dataset Limits

To process a specific number of samples (useful for testing):

```bash
uv run python scripts/preprocess_maps.py data.train_num_samples=100 data.val_num_samples=20
```

### Processing Specific Datasets

The script automatically processes both train and validation datasets. The datasets are configured in `configs/data/base.yaml`:
- Train: `data/datasets/images/train`
- Validation: `data/datasets/images_val_canonical`

## How It Works

The script performs the following steps for both the training and validation datasets:

1. **Initializes Dataset and Collate Function**: Uses Hydra to instantiate `OCRDataset` and `DBCollateFN` with the same configuration used during training
2. **Creates Output Directories**: Creates new directories (e.g., `data/datasets/images/train_maps/`, `data/datasets/images_val_canonical_maps/`) to store the generated map files
3. **Iterates Through Dataset**: Loops through every sample in the dataset (respecting `num_samples` limits)
4. **Filters Degenerate Polygons**: Removes polygons with <3 points or dimensions <1px to ensure stable pyclipper operations
5. **Generates Maps**: For each sample, calls the `make_prob_thresh_map` method to generate the `prob_map` and `thresh_map`
6. **Saves Maps**: The generated NumPy arrays are saved to a compressed `.npz` file, named to correspond with the original image file (e.g., `image_001.npz`)
7. **Validates Output**: Performs sanity checks on generated files to ensure correct shapes and dimensions

During training and validation, the modified `OCRDataset` class now loads these `.npz` files directly in `__getitem__`, bypassing the expensive on-the-fly computation.

## Output Structure

After running the pre-processing script, your directory structure will look like this:

```
data/datasets/
├── images/
│   ├── train/
│   │   ├── image_001.jpg
│   │   ├── image_002.jpg
│   │   └── ...
│   └── train_maps/          # Generated by preprocessing
│       ├── image_001.npz
│       ├── image_002.npz
│       └── ...
├── images_val_canonical/
│   ├── image_001.jpg
│   └── ...
└── images_val_canonical_maps/  # Generated by preprocessing
    ├── image_001.npz
    └── ...
```

Each `.npz` file contains:
- `prob_map`: Probability map of shape `(1, H, W)` where H, W match the transformed image dimensions
- `thresh_map`: Threshold map of shape `(1, H, W)`

## Map File Format

The `.npz` files are compressed NumPy archives containing two arrays:

```python
# Loading a map file
import numpy as np
data = np.load('data/datasets/images/train_maps/image_001.npz')
prob_map = data['prob_map']    # Shape: (1, 640, 640)
thresh_map = data['thresh_map']  # Shape: (1, 640, 640)
```

The channel dimension (first axis) is included to match the expected tensor format during training.

## Integration with Training Pipeline

The training pipeline automatically uses pre-processed maps when available:

1. **Dataset Loading** (`OCRDataset.__getitem__`):
   - Loads image and applies transforms
   - Checks for corresponding `.npz` file in `{dataset_dir}_maps/`
   - If found, loads `prob_map` and `thresh_map` and adds to item
   - If not found, maps will be generated on-the-fly (fallback behavior)

2. **Batch Collation** (`DBCollateFN.__call__`):
   - Checks each batch item for pre-loaded maps
   - If maps exist, converts to tensors and stacks
   - If maps missing, generates on-the-fly using `make_prob_thresh_map`
   - Returns collated batch with maps of shape `[batch_size, 1, H, W]`

This design ensures backward compatibility: training works even without pre-processing, though at reduced speed.

## Troubleshooting

### Memory Issues

If pre-processing fails due to RAM:
- Reduce the number of samples: `data.train_num_samples=500`
- Process datasets separately by modifying the script
- Increase swap space or use a machine with more RAM

### Missing Files

**Error**: `FileNotFoundError: Dataset directory not found`

**Solution**: Ensure datasets are prepared and accessible at the paths specified in `configs/data/base.yaml`. Check:
- `dataset_base_path` is correct
- Image directories exist (`images/train`, `images_val_canonical`)
- Annotation files exist (`jsons/train.json`, `jsons/val.json`)

### Shape Errors

**Error**: `AssertionError: Maps should have same shape`

**Solution**: This indicates a bug in map generation. Check:
- Transforms are applied consistently
- Image dimensions match between prob_map and thresh_map
- No modifications to transform pipeline between preprocessing and training

### No Maps Generated

**Warning**: `No probability maps generated for train_dataset`

**Causes**:
- Dataset has no valid polygons (all filtered out)
- Annotation file is empty or malformed
- Image files don't exist at specified paths

**Solution**: Check logs for polygon filtering messages. Ensure annotations contain valid polygon data.

### Validation Dataset Empty

If validation preprocessing shows 0 samples:
- Check that `images_val_canonical` directory exists and contains images
- Verify `val.json` annotation file exists and has entries
- Ensure `data.val_num_samples` is not set to 0

## Performance Expectations

After pre-processing, you should observe:

- **Pre-processing time**: ~1-3 minutes for 2000 training samples (depends on CPU)
- **Disk usage**: ~50-100 MB per 1000 samples (compressed .npz format)
- **Training speedup**: 5-8x faster validation epochs compared to on-the-fly generation
- **Memory usage**: Minimal increase (maps loaded individually, not cached in RAM)

## Maintenance

### Re-running Pre-processing

You need to re-run preprocessing if:
- Dataset images or annotations change
- Transform pipeline is modified (resize, augmentation parameters)
- DBCollateFN parameters change (`shrink_ratio`, `thresh_min`, `thresh_max`)

Simply re-run the script to regenerate all maps:

```bash
uv run python scripts/preprocess_maps.py
```

Existing `.npz` files will be overwritten.

### Cleaning Up

To remove all pre-processed maps:

```bash
rm -rf data/datasets/images/train_maps
rm -rf data/datasets/images_val_canonical_maps
```

Training will automatically fall back to on-the-fly generation.

## Technical Details

### Map Generation Algorithm

The probability and threshold maps are generated using the Differentiable Binarization (DB) algorithm from the paper "Real-time Scene Text Detection with Differentiable Binarization" (https://arxiv.org/pdf/1911.08947.pdf).

For each polygon:

1. **Probability Map**:
   - Shrink polygon by distance D = area × (1 - r²) / perimeter
   - Fill shrunken polygon with value 1.0
   - Default shrink ratio r = 0.4

2. **Threshold Map**:
   - Dilate polygon by distance D
   - Compute distance from each pixel to polygon edges
   - Normalize and clip to [thresh_min, thresh_max]
   - Default: thresh_min = 0.3, thresh_max = 0.7

### Polygon Filtering

Polygons are filtered out if they are degenerate:
- Fewer than 3 points (not a valid polygon)
- Width or height < 1.0 pixels (too small to be meaningful)
- Zero area (collapsed to a line or point)
- Integer span is zero (rounds to a single pixel)

These filters prevent PyClipper crashes and ensure stable training.

### Fallback Behavior

If a `.npz` file is missing or corrupted during training:
- `OCRDataset` silently skips loading (logs warning)
- `DBCollateFN` detects missing maps and generates on-the-fly
- Training continues without interruption (at reduced speed for that sample)

This ensures robustness during development and experimentation.

## Related Files

- **Pre-processing Script**: `scripts/preprocess_maps.py`
- **Dataset Class**: `ocr/datasets/base.py` (OCRDataset)
- **Collate Function**: `ocr/datasets/db_collate_fn.py` (DBCollateFN)
- **Configuration**: `configs/data/base.yaml`
- **Unit Tests**: `tests/test_preprocess_maps.py`

## See Also

- [DBNet Paper](https://arxiv.org/pdf/1911.08947.pdf) - Original algorithm
- [PyClipper Documentation](https://github.com/fonttools/pyclipper) - Polygon operations
- Polygon Pre-Processing Implementation Plan - Detailed refactoring guide
