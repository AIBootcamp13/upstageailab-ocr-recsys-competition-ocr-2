# **filename: docs/ai_handbook/02_protocols/12_streamlit_refactoring_protocol.md**
<!-- ai_cue:priority=high -->
<!-- ai_cue:use_when=streamlit,refactor -->

# **Protocol: Streamlit Refactoring**

Use this protocol when executing structural changes to any Streamlit UI module in this repository (new features, component rewrites, state management overhauls). It complements the global Modular Refactor protocol with Streamlit-specific guidance; examples call out the OCR inference app because it is the current reference implementation.

---

## **1. Architectural Snapshot**

Before touching code, align on the current layout:

```
ui/
├─ <app_entry>.py            # Thin entrypoint (do not regress to monolith)
└─ apps/<app_name>/
   ├─ app.py                 # Page lifecycle orchestrator
   ├─ components/            # Streamlit UI fragments (sidebar, results, etc.)
   ├─ models/                # Pydantic/dataclass state + UI payload validation
   ├─ services/              # Pure-Python services (catalog, schema, inference)
   ├─ state.py               # Session management helpers
   └─ ...
```

Configuration lives in `configs/ui/<app_name>.yaml`, while domain schemas sit in `configs/schemas/` (`default_model.yaml`, `ui_inference_compat.yaml`, or app-specific siblings). UI copy and demo payloads belong in `ui_meta/<app_name>/`. Any refactor must preserve or extend these contracts.

---

## **2. Preconditions**

- Baseline behaviour captured (screenshots, logs, or a short Loom video).
- `uv run streamlit run ui/<app_entry>.py --server.port=8504` confirmed working.
- Compatibility schemas (`configs/schemas/*.yaml`) clean (no outstanding issues in the sidebar).
- Tests/lint passing (`uv run ruff check ui/apps/<app_name> ui/utils`).
- Pydantic models in `ui/apps/<app_name>/models/` updated to reflect any schema changes (Pydantic is for validating UI/session payloads only; Hydra/OmegaConf still own runtime configuration).

---

## **3. Refactor Planning Checklist**

1. **Define the Delta**
   - What UX or architectural change is desired?
   - Which module boundaries are impacted (component vs. service vs. config)?
2. **Document Contracts**
   - Note inputs/outputs for affected services (e.g., `InferenceService.run`).
   - Capture Streamlit widget keys that must remain stable for session state.
3. **Identify External Touchpoints**
   - Config keys in `configs/ui/<app_name>.yaml`
   - Schema families in `configs/schemas/ui_inference_compat.yaml` (and related schemas such as `default_model.yaml`)
   - UI metadata in `ui_meta/<app_name>/`
   - CLI wrappers (`ui-infer` alias, documentation references)
4. **Decide Migration Strategy**
   - Phased (feature flags via config)
   - Big-bang (rare; only if test coverage is strong)

---

## **4. Execution Phases**

### **Phase A: Isolate Changes**

- Fork new module(s) within the appropriate package (`components/` or `services/`).
- Keep pure logic in services; UI code should remain declarative and layout focused.
- Add or update dataclasses in `models/` when introducing fresh state.

### **Phase B: Wire Config & Schema**

- Surface new toggles in `configs/ui/<app_name>.yaml`. Avoid hard-coded literals and keep help text in sync with `ui_meta/` copy decks.
- Expand schema families (e.g., `configs/schemas/ui_inference_compat.yaml` for OCR) when introducing new decoder/head combos; run catalogue validation to confirm.
- Update `configs/schemas/default_model.yaml` if default training/inference assumptions change.
- Keep Pydantic models limited to UI/session validation; never try to mirror Hydra configs in Pydantic.

### **Phase C: Update Entry Surfaces**

- Adjust `app.py` orchestration (e.g., new sidebar step) while keeping caching boundaries (`@st.cache_data` / `@st.cache_resource`).
- Ensure `InferenceState` persists new session data safely across reruns.

### **Phase D: Verify Behaviour**

- Run manual smoke tests:
  1. Launch UI
  2. Load multiple checkpoints
  3. Exercise inference (single + batch)
- Validate logging output (`INFO` and `ERROR` lines) for clarity.
- Confirm fallback paths (mock inference, empty catalogue) still work.

---

## **5. Testing Strategy**

1. **Unit-ish Checks**
   - `uv run python - <<'PY' ... build_catalog(...)`
   - `uv run python - <<'PY' ... InferenceEngine().load_model(...)`
2. **Streamlit Smoke**
   - Local manual run (screen-share or screenshot for PR)
   - Optional: Autogenerated screenshot via `streamlit_static_snapshot` if available.
3. **Regression Guard**
   - Capture performance metrics if you touch caching or inference loops (log inference duration before/after).

---

## **6. Refactor Completion Gate**

- [ ] UI renders without warnings and respects new config values.
- [ ] Real data path succeeds (for OCR: at least one checkpoint per encoder family).
- [ ] Compatibility/schema files updated with any new families or defaults (`ui_inference_compat.yaml`, `default_model.yaml`, etc.).
- [ ] Documentation: this protocol referenced in PR, changelog entry added.
- [ ] Legacy entrypoint (`ui/inference_ui.py`) still delegates to `app.main` only.
- [ ] Agentic AI prompts (e.g., `project-overview.md`) mention major UI capability shifts.

---

## **7. Common Pitfalls & Mitigations**

| Pitfall | Mitigation |
| --- | --- |
| Re-introducing monolithic logic in `ui/<app_entry>.py` | Keep file as thin proxy; all logic belongs under `ui/apps/<app_name>/`. |
| Breaking cache keys causing stale state | Explicitly set `ttl` or include config versions in cache key inputs. |
| Forgetting schema updates for new checkpoints or assets | Add a family entry in `configs/schemas/*.yaml` and rerun catalogue validation. |
| Hard-coded UI strings without config sync | Update `configs/ui/<app_name>.yaml` and matching `ui_meta/<app_name>/` copy. |
| Using Pydantic to replace Hydra/OmegaConf configs | Restrict Pydantic models to validating submission/session payloads; keep Hydra configs canonical. |
| Mock fallback hiding real failures | Inspect logs; ensure service layers only fall back on explicit exceptions. |
| Insufficient testing after modular refactoring | Create unit tests for each module, integration tests for UI components, and regression tests for fixed issues. See `command_builder_testing_guide.md` for examples. |

---

## **8. Deliverables for Every Refactor**

1. Code changes + config/schema updates.
2. Updated docs: maintenance protocol (if process changed) and changelog.
3. Verification evidence (logs, screenshots, benchmark numbers).
4. PR checklist referencing this document and the global modular refactor protocol.

Use this protocol to keep Streamlit refactors disciplined, reproducible, and pleasant for future Agentic AI collaborators.
