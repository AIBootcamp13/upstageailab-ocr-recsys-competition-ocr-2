"""Pydantic models defining data contracts for dataset transforms."""

from __future__ import annotations

from pathlib import Path
from typing import Any

import numpy as np
import torch
from pydantic import BaseModel, ConfigDict, Field, field_validator


class ImageMetadata(BaseModel):
    """Metadata describing the context of an image being transformed."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    filename: str | None = None
    path: Path | None = None
    original_shape: tuple[int, int]
    orientation: int = Field(ge=0, le=8, default=1)
    is_normalized: bool = False
    dtype: str
    raw_size: tuple[int, int] | None = None
    polygon_frame: str | None = None
    cache_source: str | None = None
    cache_hits: int | None = Field(default=None, ge=0)
    cache_misses: int | None = Field(default=None, ge=0)

    @field_validator("original_shape")
    @classmethod
    def validate_original_shape(cls, value: tuple[int, int]) -> tuple[int, int]:
        if len(value) != 2:
            raise ValueError("original_shape must be a tuple of (height, width)")
        height, width = value
        return (int(height), int(width))

    @field_validator("raw_size")
    @classmethod
    def validate_raw_size(cls, value: tuple[int, int] | None) -> tuple[int, int] | None:
        if value is None:
            return None
        if len(value) != 2:
            raise ValueError("raw_size must be a tuple of (width, height)")
        width, height = value
        return (int(width), int(height))


class PolygonData(BaseModel):
    """Validated polygon representation with consistent shape."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    points: np.ndarray
    confidence: float | None = None
    label: str | None = None

    @field_validator("points", mode="before")
    @classmethod
    def validate_points(cls, value: Any) -> np.ndarray:
        if not isinstance(value, np.ndarray):
            value = np.asarray(value, dtype=np.float32)

        if value.ndim == 1 and value.size % 2 == 0:
            value = value.reshape(-1, 2)
        elif value.ndim == 3 and value.shape[0] == 1:
            value = value.squeeze(0)

        if value.ndim != 2 or value.shape[1] != 2:
            raise ValueError(f"Polygon must be (N, 2) array, got shape {value.shape}")

        if value.shape[0] < 3:
            raise ValueError(f"Polygon must have at least 3 points, got {value.shape[0]}")

        return value.astype(np.float32)


class TransformInput(BaseModel):
    """Input payload for the OCR transform pipeline."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    image: np.ndarray
    polygons: list[PolygonData] | None = None
    metadata: ImageMetadata | None = None

    @field_validator("image", mode="before")
    @classmethod
    def validate_image(cls, value: Any) -> np.ndarray:
        if not isinstance(value, np.ndarray):
            raise TypeError(f"Image must be numpy array, got {type(value)}")

        if value.ndim not in (2, 3):
            raise ValueError(f"Image must be 2D or 3D, got {value.ndim}D")

        if value.ndim == 3 and value.shape[2] not in (1, 3):
            raise ValueError(f"Image must have 1 or 3 channels, got {value.shape[2]}")

        return value


class TransformOutput(BaseModel):
    """Validated output generated by the OCR transform pipeline."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    image: torch.Tensor
    polygons: list[np.ndarray]
    inverse_matrix: np.ndarray
    metadata: dict[str, Any] | None = None

    @field_validator("image", mode="before")
    @classmethod
    def validate_output_image(cls, value: Any) -> torch.Tensor:
        if not isinstance(value, torch.Tensor):
            raise TypeError(f"Output image must be torch.Tensor, got {type(value)}")

        if value.ndim != 3:
            raise ValueError(f"Output image must be 3D (C, H, W), got shape {tuple(value.shape)}")

        return value

    @field_validator("polygons", mode="before")
    @classmethod
    def validate_output_polygons(cls, value: Any) -> list[np.ndarray]:
        if value is None:
            return []

        if not isinstance(value, list):
            raise TypeError(f"Output polygons must be list, got {type(value)}")

        normalized: list[np.ndarray] = []
        for idx, polygon in enumerate(value):
            if not isinstance(polygon, np.ndarray):
                polygon = np.asarray(polygon, dtype=np.float32)

            if polygon.ndim == 2:
                polygon = polygon.reshape(1, -1, 2)

            if polygon.shape[0] != 1 or polygon.shape[2] != 2:
                raise ValueError(f"Polygon at index {idx} must have shape (1, N, 2), got {polygon.shape}")

            normalized.append(polygon.astype(np.float32))

        return normalized

    @field_validator("inverse_matrix", mode="before")
    @classmethod
    def validate_matrix(cls, value: Any) -> np.ndarray:
        if not isinstance(value, np.ndarray):
            value = np.asarray(value, dtype=np.float32)

        if value.shape != (3, 3):
            raise ValueError(f"Inverse matrix must be (3, 3), got {value.shape}")

        return value.astype(np.float32)


class TransformConfig(BaseModel):
    """Configuration for image normalization and transform probabilities."""

    mean: tuple[float, float, float] = (0.485, 0.456, 0.406)
    std: tuple[float, float, float] = (0.229, 0.224, 0.225)
    always_apply: bool = False
    p: float = 1.0

    @field_validator("mean", "std")
    @classmethod
    def validate_mean_std(cls, value: tuple[float, ...]) -> tuple[float, float, float]:
        if len(value) != 3:
            raise ValueError("Mean and std must each provide 3 values for RGB channels")
        return tuple(float(v) for v in value)  # type: ignore[return-value]
