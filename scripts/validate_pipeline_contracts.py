#!/usr/bin/env python3
"""
Pipeline Contract Validation Script

Validates that all OCR pipeline components maintain data contracts.
Run this before committing changes to prevent contract violations.

Usage:
    python scripts/validate_pipeline_contracts.py
    python scripts/validate_pipeline_contracts.py --verbose
    python scripts/validate_pipeline_contracts.py --component dataset
"""

import argparse
import sys
from pathlib import Path
from typing import Any

import numpy as np
import torch
from PIL import Image

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import albumentations as A

from ocr.datasets import DBCollateFN, OCRDataset
from ocr.datasets.transforms import DBTransforms


class ContractValidator:
    """Validates data contracts across OCR pipeline components."""

    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.errors: list[str] = []
        self.warnings: list[str] = []

    def log(self, message: str) -> None:
        """Log message if verbose mode."""
        if self.verbose:
            print(message)

    def error(self, message: str) -> None:
        """Record an error."""
        self.errors.append(message)
        print(f"❌ ERROR: {message}")

    def warning(self, message: str) -> None:
        """Record a warning."""
        self.warnings.append(message)
        print(f"⚠️  WARNING: {message}")

    def success(self, message: str) -> None:
        """Log success message."""
        self.log(f"✅ {message}")

    def validate_dataset_contract(self, sample: dict[str, Any]) -> bool:
        """Validate OCRDataset output contract."""
        self.log("Validating dataset contract...")

        # Required keys (maps are generated by collate function, not dataset)
        required_keys = ["image", "polygons", "image_filename", "image_path", "inverse_matrix", "shape"]

        for key in required_keys:
            if key not in sample:
                self.error(f"Missing required key: {key}")
                return False

        # Image validation (can be numpy array or torch tensor after transforms)
        image = sample["image"]
        if not isinstance(image, np.ndarray | torch.Tensor):
            self.error(f"Image must be numpy array or torch tensor, got {type(image)}")
            return False

        if isinstance(image, torch.Tensor):
            # For torch tensors, shape is (C, H, W)
            if image.ndim != 3:
                self.error(f"Image tensor must be 3D (C, H, W), got {image.ndim}D")
                return False
            if image.shape[0] != 3:
                self.error(f"Image tensor must have 3 channels, got {image.shape[0]}")
                return False
        else:
            # For numpy arrays, shape is (H, W, C)
            if image.ndim != 3:
                self.error(f"Image array must be 3D (H, W, C), got {image.ndim}D")
                return False
            if image.shape[2] != 3:
                self.error(f"Image array must have 3 channels, got {image.shape[2]}")
                return False

        # Polygon validation
        polygons = sample["polygons"]
        if not isinstance(polygons, list):
            self.error(f"Polygons must be list, got {type(polygons)}")
            return False

        for i, polygon in enumerate(polygons):
            if not isinstance(polygon, np.ndarray):
                self.error(f"Polygon {i} must be numpy array, got {type(polygon)}")
                return False

            # Accept both 2D (N, 2) and 3D (1, N, 2) shapes from transforms
            if polygon.ndim == 2:
                # Shape should be (N, 2)
                if polygon.shape[1] != 2:
                    self.error(f"Polygon {i} must have 2 coordinates, got {polygon.shape[1]}")
                    return False
            elif polygon.ndim == 3:
                # Shape should be (1, N, 2) for batched polygons
                if polygon.shape[0] != 1 or polygon.shape[2] != 2:
                    self.error(f"Polygon {i} 3D shape must be (1, N, 2), got {polygon.shape}")
                    return False
            else:
                self.error(f"Polygon {i} must be 2D or 3D, got {polygon.ndim}D")
                return False

            # Check minimum points (get N from appropriate dimension)
            num_points = polygon.shape[1] if polygon.ndim == 2 else polygon.shape[1]
            if num_points < 3:
                self.warning(f"Polygon {i} has only {num_points} points (minimum 3)")

        # Optional map validation (maps are generated by collate function if not present)
        for map_name in ["prob_maps", "thresh_maps"]:
            if map_name in sample:
                map_data = sample[map_name]
                if not isinstance(map_data, np.ndarray):
                    self.error(f"{map_name} must be numpy array, got {type(map_data)}")
                    return False

                if map_data.ndim != 2:
                    self.error(f"{map_name} must be 2D (H, W), got {map_data.ndim}D")
                    return False

                # Get image dimensions based on type
                if isinstance(image, torch.Tensor):
                    # Torch tensor shape is (C, H, W)
                    image_height, image_width = image.shape[1], image.shape[2]
                else:
                    # Numpy array shape is (H, W, C)
                    image_height, image_width = image.shape[0], image.shape[1]

                if map_data.shape != (image_height, image_width):
                    self.error(f"{map_name} shape {map_data.shape} != image shape ({image_height}, {image_width})")
                    return False

                if not np.all((map_data >= 0) & (map_data <= 1)):
                    self.warning(f"{map_name} values outside [0, 1] range")

        self.success("Dataset contract validation passed")
        return True

    def validate_transform_contract(self, input_data: dict[str, Any], output_data: dict[str, Any]) -> bool:
        """Validate DBTransforms contract."""
        self.log("Validating transform contract...")

        # Input validation
        if "image" not in input_data or "polygons" not in input_data:
            self.error("Transform input missing required keys")
            return False

        # Output validation
        required_output_keys = ["image", "polygons"]
        for key in required_output_keys:
            if key not in output_data:
                self.error(f"Transform output missing required key: {key}")
                return False

        # Output image should be torch tensor
        output_image = output_data["image"]
        if not isinstance(output_image, torch.Tensor):
            self.error(f"Transform output image must be torch tensor, got {type(output_image)}")
            return False

        if output_image.ndim != 3:
            self.error(f"Transform output image must be 3D (C, H, W), got {output_image.ndim}D")
            return False

        if output_image.shape[0] != 3:
            self.error(f"Transform output image must have 3 channels, got {output_image.shape[0]}")
            return False

        self.success("Transform contract validation passed")
        return True

    def validate_collate_contract(self, batch: dict[str, Any]) -> bool:
        """Validate DBCollateFN contract."""
        self.log("Validating collate contract...")

        # Required keys
        required_keys = [
            "images",
            "polygons",
            "prob_maps",
            "thresh_maps",
            "image_filename",
            "image_path",
            "inverse_matrix",
            "raw_size",
            "orientation",
            "canonical_size",
        ]

        for key in required_keys:
            if key not in batch:
                self.error(f"Collate output missing required key: {key}")
                return False

        # Batch tensors should have consistent batch dimension
        batch_size = batch["images"].shape[0]

        for tensor_name in ["images", "prob_maps", "thresh_maps"]:
            tensor = batch[tensor_name]
            if not isinstance(tensor, torch.Tensor):
                self.error(f"{tensor_name} must be torch tensor, got {type(tensor)}")
                return False

            if tensor.shape[0] != batch_size:
                self.error(f"{tensor_name} batch size {tensor.shape[0]} != expected {batch_size}")
                return False

        # Single fields (not batched)
        single_fields = ["image_filename", "image_path", "inverse_matrix", "raw_size", "orientation", "canonical_size"]
        for field_name in single_fields:
            if not isinstance(batch[field_name], list):
                self.error(f"{field_name} should be a list for batching, got {type(batch[field_name])}")
                return False
            if len(batch[field_name]) != batch_size:
                self.error(f"{field_name} length {len(batch[field_name])} != batch size {batch_size}")
                return False

        # Polygons field should match batch size
        if len(batch["polygons"]) != batch_size:
            self.error(f"polygons length {len(batch['polygons'])} != batch size {batch_size}")
            return False

        # Polygon validation
        for i, polygons in enumerate(batch["polygons"]):
            if not isinstance(polygons, list):
                self.error(f"Sample {i} polygons must be list, got {type(polygons)}")
                return False

            for j, polygon in enumerate(polygons):
                if not isinstance(polygon, np.ndarray):
                    self.error(f"Sample {i} polygon {j} must be numpy array, got {type(polygon)}")
                    return False

                if polygon.ndim != 2 or polygon.shape[1] != 2:
                    self.error(f"Sample {i} polygon {j} must be (N, 2), got {polygon.shape}")
                    return False

        self.success("Collate contract validation passed")
        return True

    def create_test_data(self) -> tuple[OCRDataset, DBTransforms, DBCollateFN]:
        """Create test dataset and components for validation."""
        # Create minimal test data
        test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
        test_polygons = [np.array([[50, 50], [150, 50], [150, 150], [50, 150]], dtype=np.float32)]
        np.random.rand(224, 224).astype(np.float32)
        np.random.rand(224, 224).astype(np.float32)

        # Create temporary directory and files
        import json
        import tempfile

        temp_dir = tempfile.mkdtemp()
        img_filename = "test_image.jpg"
        img_path = Path(temp_dir) / img_filename

        # Create temporary image file
        Image.fromarray(test_image).save(img_path)

        # Create temporary annotation file
        annotations = {"images": {img_filename: {"words": {"word1": {"points": test_polygons[0].tolist()}}}}}

        anno_path = Path(temp_dir) / "annotations.json"
        with open(anno_path, "w") as f:
            json.dump(annotations, f)

        # Create components
        transforms = DBTransforms(transforms=[A.Resize(224, 224)], keypoint_params=A.KeypointParams(format="xy", remove_invisible=False))

        dataset = OCRDataset(image_path=temp_dir, annotation_path=str(anno_path), transform=transforms, preload_images=True)

        collate_fn = DBCollateFN()

        # Cleanup temp directory will happen automatically
        import atexit
        import shutil

        atexit.register(lambda: shutil.rmtree(temp_dir, ignore_errors=True))

        return dataset, transforms, collate_fn

    def validate_full_pipeline(self) -> bool:
        """Validate complete pipeline contracts."""
        self.log("🔍 Starting full pipeline contract validation...")

        try:
            # Create test components
            dataset, transforms, collate_fn = self.create_test_data()

            # Test dataset contract
            sample = dataset[0]
            if not self.validate_dataset_contract(sample):
                return False

            # Test transform contract (transforms are applied in dataset)
            # Note: transforms are applied during dataset.__getitem__

            # Test collate contract
            batch = collate_fn([sample])
            if not self.validate_collate_contract(batch):
                return False

            self.success("Full pipeline contract validation PASSED")
            return True

        except Exception as e:
            self.error(f"Pipeline validation failed with exception: {e}")
            if self.verbose:
                import traceback

                traceback.print_exc()
            return False

    def validate_component(self, component: str) -> bool:
        """Validate specific component."""
        if component == "dataset":
            dataset, _, _ = self.create_test_data()
            sample = dataset[0]
            return self.validate_dataset_contract(sample)
        elif component == "collate":
            _, _, collate_fn = self.create_test_data()
            dataset, _, _ = self.create_test_data()
            sample = dataset[0]
            batch = collate_fn([sample])
            return self.validate_collate_contract(batch)
        else:
            self.error(f"Unknown component: {component}")
            return False

    def report(self) -> bool:
        """Generate validation report."""
        print("\n" + "=" * 60)
        print("📋 CONTRACT VALIDATION REPORT")
        print("=" * 60)

        if not self.errors and not self.warnings:
            print("✅ ALL CONTRACTS VALID - No errors or warnings")
            return True

        if self.errors:
            print(f"❌ {len(self.errors)} ERRORS found:")
            for error in self.errors:
                print(f"   • {error}")
            print()

        if self.warnings:
            print(f"⚠️  {len(self.warnings)} WARNINGS found:")
            for warning in self.warnings:
                print(f"   • {warning}")
            print()

        print("🔗 See docs/pipeline/data_contracts.md for contract specifications")
        print("🔧 See docs/troubleshooting/shape_issues.md for fixing guidance")

        return len(self.errors) == 0


def main():
    parser = argparse.ArgumentParser(description="Validate OCR pipeline data contracts")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    parser.add_argument("--component", "-c", choices=["dataset", "collate", "full"], default="full", help="Component to validate")

    args = parser.parse_args()

    validator = ContractValidator(verbose=args.verbose)

    if args.component == "full":
        success = validator.validate_full_pipeline()
    else:
        success = validator.validate_component(args.component)

    validator.report()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
if __name__ == "__main__":
    main()
